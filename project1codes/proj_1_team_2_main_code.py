# -*- coding: utf-8 -*-
"""Proj_1_Team_2_Main_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YHba-ecJ4ChH1oRYowuiFNfFLXQWAAyb

Project 1 : VMs, Database and Scripting

Including the Python Code for:

*   Part 1: Web Scraping City data
*   Part 2: API Call for Weather data
*   Part 3:  API Call for Picture
*   Part 4: Store the collected data into MySQL DB
"""

from bs4 import BeautifulSoup
import requests
import pandas as pd

# Function: Scrape a single City page
def scrape_single_city_page(soup, city_list):
  """
  Function to Scrap a single page
  Scrape the city page content given by Soup object <soup>
  The found cities are stored in the list <city_list>
  """

  print('Scrap Single City page')

  # Get City table:
  city_tab = soup.find('table', class_='table table-hover table-bordered')

  for row in city_tab.find_all('tr'):
    cols = row.find_all('td')

    if cols:
      # Structure of City table:
      # [0] City number
      # [1] City name
      # [2] Country
      # [3] Latitude
      # [4] Longitude

      city_list.append(
            {
                'city_number': cols[0].text.strip(),
                'city_name': cols[1].text.strip(),
                'country': cols[2].text.strip(),
                'latitude': float(cols[3].text.strip()),
                'longitude': float(cols[4].text.strip())
            })

# Function: Get Next Page within City Main URL
def get_next_page(soup, main_url):
  """
  Function to get the next page for the URL <main_url>
  Return : The URL for the next page
  """
  print('Search for next page')

  next_page_url = None
  next_page_link = soup.find_all('a', class_='page-link')
  for link in next_page_link:
    if link.text == "Next":
      print('Next page:')
      print(link['href'])
      # Get next page for main URL
      if link['href'].startswith(main_url) :
        next_page_url = link['href']
  return next_page_url

# Function Scrape all pages for the City Main URL
def scrape_all_city_data(main_url):
  '''
  Scrap all city pages for the URL <main_url>

  Return: Dataframe <df> which contains the City data
  '''
  print('Scrap all City pages')
  html_req = requests.get(main_url)
  soup = BeautifulSoup(html_req.text, 'html.parser')

  # Main list objects which collects the City metadata
  city_list = []

  # Scrap the first page
  scrape_single_city_page(soup, city_list)

  # Check if a next page is available
  next_page_url = get_next_page(soup, main_url)

  # Traverse through the next pages to collect
  # the data for all cities
  while next_page_url is not None:

    # Take the next page, scrap it and (if available) search for the (overnext) page
    html_req = requests.get(next_page_url)
    soup = BeautifulSoup(html_req.text, 'html.parser')
    scrape_single_city_page(soup, city_list)
    next_page_url = get_next_page(soup, main_url)

  # Store the results in a DataFrame
  df = pd.DataFrame(city_list)

  return df

def get_weather_data(latitude, longitude, appid):
  weather_data = dict()


  # Define the URL
  # Uncomment this line and please use this carefully: the appid is limited to 1,000,000 calls/month and 60 calls/minute
  url = f"https://api.openweathermap.org/data/2.5/weather?lat={str(latitude)}&lon={str(longitude)}&units=metric&appid={appid}"

  # Make the request and get the response object
  response = requests.get(url)

  # Check if the request was successful
  if response.status_code == 200:
    # Parse the JSON content from the response
    data = response.json()
  else:
    print(f"Failed to retrieve data, status code: {response.status_code}")
    return weather_data

  weather_data['temperature'] = data['main']['temp']
  weather_data['weather'] = data['weather'][0]['main']
  weather_data['weather_description'] = data['weather'][0]['description']

  return weather_data

from google.cloud import storage
from PIL import Image

def get_city_pic_download_url(city_name, client_id):

  bucket_name = "bucket-project-group2"
  destination_blob_name = city_name + ".jpg"
  source_file_name = "./" + destination_blob_name
  url = f"https://api.unsplash.com/photos/random?query={city_name}&client_id={client_id}"

  # Google bucket connection
  storage_client = storage.Client()
  bucket = storage_client.bucket(bucket_name)
  blob = bucket.blob(destination_blob_name)

  # Check if the image already in the bucket
  if blob.exists():
    link = "https://storage.cloud.google.com/bucket-project-group2/" + destination_blob_name
    return link

  # Make the request and get the response object
  response = requests.get(url)

  # Check if the request was successful
  if response.status_code == 200:
    # Parse the JSON content from the response
    data = response.json()
  else:
    print(f"Failed to retrieve data, status code: {response.status_code}")
    return ''

  # Get the download url of the picture
  pic_url = data['links']['download']

  # Download the picture to the server
  response = requests.get(pic_url, allow_redirects=True)
  f = open(source_file_name, 'wb').write(response.content)

  image = Image.open(source_file_name)
  image.thumbnail((1000, 1000))
  image.save(source_file_name)

  # Upload the picture to bucket
  generation_match_precondition = 0

  #blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)

  print(f"File {source_file_name} uploaded to {destination_blob_name}.")
  link = "https://storage.cloud.google.com/bucket-project-group2/" + destination_blob_name

  return link

def older_get_city_pic_download_url(city_name, client_id):

  url = f"https://api.unsplash.com/photos/random?query={city_name}&client_id={client_id}"


  # Make the request and get the response object
  response = requests.get(url)

  # Check if the request was successful
  if response.status_code == 200:
    # Parse the JSON content from the response
    data = response.json()
  else:
    print(f"Failed to retrieve data, status code: {response.status_code}")
    return ''

  # Get the download url of the picture
  pic_url = data['links']['download']

  return pic_url

# Main program
# Overall parameters - needed for API Call
# Unsplash API:
unsplash_client_id = 'ty6H68858-Z1NCWYpPX4GzPKoq-b-oMMaN3pl8LRkCE'

# OpenWeather API:
openweather_appid = 'e88b8a7f2403142335eb142bbbdceed5'

# City URLs
# Germany
main_url="https://geokeo.com/database/city/de"
# Hungary
#main_url="https://geokeo.com/database/region/hu/"

print('Start main program')
print(f"Scrap {main_url}")

html_req = requests.get(main_url)
soup = BeautifulSoup(html_req.text, 'html.parser')

# Scrape the City data for main URL
df = None
df = scrape_all_city_data(main_url)

# Calculate the Weather Data for the Cities
# Function provides a Dictonary which is split in 3 separate columns in the target DataFrame:
# - temperature
# - weather
# - weather description
# Value for additional (3.) Call parameter (appid) is set beforehand in Main script

print('Get Weather data')
df[['temperature', 'weather', 'weather_desc']] = df.apply(lambda row, appid=openweather_appid: get_weather_data(row['latitude'], row['longitude'], appid), axis=1, result_type='expand')

print('Get City pictures')
# Get the URLs for the City pictures
df['pic_url'] = df.apply(lambda row, cliend_id=unsplash_client_id: get_city_pic_download_url(row['city_name'], cliend_id), axis=1)

print('End main program')

#df

"""**Update the MySQL Database**

Caution: Currently only working on VM
"""

# Function get Connection to MySQL DB on GCP VM
def get_db_connection_on_vm() :

  import mysql.connector
  INSTCONN_NAME = f"{'project-group2-411509'}:{'europe-west3'}:{'sql-project'}"
  DB_USER = "root"
  DB_PASS = "Start!789"
  DB_NAME = "Data_DB"
  #HOST = "34.107.67.171"
  # Private IP Adddress of MySQL DB:
  HOST = "10.3.16.3"
  print('InstConnName', INSTCONN_NAME)
  cnx = mysql.connector.connect(
    user = DB_USER,
    password = DB_PASS,
    host = HOST, #Cloud SQL instance internal IP address
    database = DB_NAME
    )

  return cnx

def insert_db_table(environment, df):
  if environment == 'VM':
    cnx = get_db_connection_on_vm()
  else:
    cnx = get_db_connection_on_colab()

  cursor = cnx.cursor()

  insert_stmt = (
    "INSERT INTO weather (city_number, city_name, country, latitude, longitude, temperature, weather, weather_desc, pic_url) "
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)"
  )

  cursor.execute("DELETE FROM weather")
  cnx.commit()

  for i, row in df.iterrows():
        data = (row['city_number'], row['city_name'], row['country'], row['latitude'], row['longitude'],
                row['temperature'], row['weather'], row['weather_desc'], row['pic_url'])
        cursor.execute(insert_stmt, data)
  cnx.commit()
  print(f"{cursor.rowcount} rows inserted.")


  cursor.close()
  cnx.close()

def select_db_table(environment, df):
  if environment == 'VM':
    cnx = get_db_connection_on_vm()
  else:
    cnx = get_db_connection_on_colab()


  cursor = cnx.cursor()

  query = ("SELECT * from weather "
         )
  cursor.execute(query)

  for row in cursor.fetchall():
      print(row)

  cursor.close()
  cnx.close()

# Insert the city data into table
environment = 'VM'
#environment = 'COLAB'

print('Insert in MySQL DB')

insert_db_table(environment, df)