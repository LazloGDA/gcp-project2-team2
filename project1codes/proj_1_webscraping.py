# -*- coding: utf-8 -*-
"""Proj_1_WebScraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yZhsSRDngQ4hGD9aE48CeLiYZLDtoGlz

**Project 1 : VMs, Database and Scripting**

Part 1: Web Scraping

Website to scrap:
https://geokeo.com/database/city/de
"""

from bs4 import BeautifulSoup
import requests
import pandas as pd

# Functions

def scrape_city_page(soup, city_list):
  """
  Function to Scrap a single page
  Scrape the city page content given by Soup object <soup>
  The found cities are stored in the list <city_list>
  """
  # Get City table:
  city_tab = soup.find('table', class_='table table-hover table-bordered')

  for row in city_tab.find_all('tr'):
    cols = row.find_all('td')

    if cols:
      # Structure of City table:
      # [0] City number
      # [1] City name
      # [2] Country
      # [3] Latitude
      # [4] Longitude

      city_list.append(
            {
                'city_number': cols[0].text.strip(),
                'city_name': cols[1].text.strip(),
                'country': cols[2].text.strip(),
                'latitude': float(cols[3].text.strip()),
                'longitude': float(cols[4].text.strip())
            })


def get_next_page(soup, main_url):
  """
  Function to get the next page for the URL <main_url>
  Return : The URL for the next page
  """
  next_page_url = None
  next_page_link = soup.find_all('a', class_='page-link')
  for link in next_page_link:
    if link.text == "Next":
      print('Next page:')
      print(link['href'])
      # Get next page for main URL
      if link['href'].startswith(main_url) :
        next_page_url = link['href']
  return next_page_url

# Main program
# Germany
main_url="https://geokeo.com/database/city/de"
# Hungary
#main_url="https://geokeo.com/database/region/hu/"

html_req = requests.get(main_url)
soup = BeautifulSoup(html_req.text, 'html.parser')

# Main list objects which collects the City metadata
city_list = []

# Scrap the first page
scrape_city_page(soup, city_list)

# Check if a next page is available
next_page_url = get_next_page(soup, main_url)

# Traverse through the next pages to collect
# the data for all cities
while next_page_url is not None:

  # Take the next page, scrap it and (if available) search for the (overnext) page
  html_req = requests.get(next_page_url)
  soup = BeautifulSoup(html_req.text, 'html.parser')
  scrape_city_page(soup, city_list)
  next_page_url = get_next_page(soup, main_url)

# Store the results in a DataFrame
df = pd.DataFrame(city_list)
df

